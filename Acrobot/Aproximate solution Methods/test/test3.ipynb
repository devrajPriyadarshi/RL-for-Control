{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"7d9d6998194bbd8c449c23903b14dd3eb7169db2c7066c14bbfe3a3189de9d7f"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install gymnasium[classic-control]","metadata":{"execution":{"iopub.status.busy":"2022-11-11T18:04:54.459507Z","iopub.execute_input":"2022-11-11T18:04:54.460096Z","iopub.status.idle":"2022-11-11T18:05:02.993130Z","shell.execute_reply.started":"2022-11-11T18:04:54.460040Z","shell.execute_reply":"2022-11-11T18:05:02.991684Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gymnasium[classic-control] in /opt/conda/lib/python3.7/site-packages (0.26.3)\n\u001b[33mWARNING: gymnasium 0.26.3 does not provide the extra 'classic-control'\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from gymnasium[classic-control]) (1.21.6)\nRequirement already satisfied: importlib-metadata>=4.8.0 in /opt/conda/lib/python3.7/site-packages (from gymnasium[classic-control]) (4.13.0)\nRequirement already satisfied: gymnasium-notices>=0.0.1 in /opt/conda/lib/python3.7/site-packages (from gymnasium[classic-control]) (0.0.1)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gymnasium[classic-control]) (2.1.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gymnasium[classic-control]) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gymnasium[classic-control]) (3.8.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport gymnasium as gym\nimport math","metadata":{"execution":{"iopub.status.busy":"2022-11-11T18:05:02.998619Z","iopub.execute_input":"2022-11-11T18:05:02.998938Z","iopub.status.idle":"2022-11-11T18:05:03.552394Z","shell.execute_reply.started":"2022-11-11T18:05:02.998902Z","shell.execute_reply":"2022-11-11T18:05:03.551396Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class PolicyNet(nn.Module):\n    def __init__(self):\n        super(PolicyNet, self).__init__()\n\n        self.fc1 = nn.Linear(4, 24)\n        self.fc2 = nn.Linear(24, 36)\n        self.fc3 = nn.Linear(36, 3)  # Prob of Left\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.sigmoid(self.fc3(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-11-11T18:05:03.553900Z","iopub.execute_input":"2022-11-11T18:05:03.554430Z","iopub.status.idle":"2022-11-11T18:05:03.560644Z","shell.execute_reply.started":"2022-11-11T18:05:03.554401Z","shell.execute_reply":"2022-11-11T18:05:03.559510Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def transformState(S_t):\n    return [ math.atan2( S_t[0], S_t[1]), math.atan2( S_t[2], S_t[3]), S_t[4], S_t[5] ]\n\nepisode_durations = []\nepisode_rewards = []\nepisode_success = []\n\ndef main():\n\n    # Parameters\n    num_episode = 1000\n    batch_size = 5\n    learning_rate = 0.1\n    gamma = 0.99\n\n    env = gym.make('Acrobot-v1')\n    policy_net = PolicyNet()\n    policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=learning_rate)\n\n    # Batch History\n    state_pool = []\n    action_pool = []\n    reward_pool = []\n    steps = 0\n\n\n    for e in range(num_episode):\n\n        state = env.reset()[0]\n        state = np.array(transformState(state))\n        state = torch.from_numpy(state).float()\n        state = Variable(state)\n\n        for t in range(1000):\n\n            probs = policy_net(state)\n            m = Categorical(probs)\n            action = m.sample()\n\n            action = action.data.numpy().astype(int)\n            stepData = env.step(action)\n            next_state, reward, done, truncated, info = stepData[0], stepData[1], stepData[2], stepData[3], stepData[4]\n            next_state = np.array(transformState(next_state))\n\n            # To mark boundarys between episodes\n            if done:\n                reward = 0\n\n            state_pool.append(state)\n            action_pool.append(float(action))\n            reward_pool.append(reward)\n\n            state = next_state\n            state = torch.from_numpy(state).float()\n            state = Variable(state)\n\n            steps += 1\n\n            if done:\n                episode_durations.append(t + 1)\n                print(\"Episode:\",e,\"\\tDuration:\",t+1)\n                episode_rewards.append(-t)\n\n                break\n\n        # Update policy\n        if e > 0 and e % batch_size == 0:\n\n            # Discount reward\n            return_G = 0\n            for i in reversed(range(steps)):\n                if reward_pool[i] == 0:\n                    return_G = 0\n                else:\n                    return_G = return_G * gamma + reward_pool[i]\n                    reward_pool[i] = return_G\n\n            # print(reward_pool)\n            # print(len(reward_pool))\n\n            # Normalize reward\n            # reward_mean = np.mean(reward_pool)\n            # reward_std = np.std(reward_pool)\n            # for i in range(steps):\n            #     reward_pool[i] = (reward_pool[i] - reward_mean) / reward_std\n\n            # Gradient Desent\n            policy_optimizer.zero_grad()\n\n            for i in range(steps):\n                state = state_pool[i]\n                action = Variable(torch.FloatTensor([action_pool[i]]))\n                reward = reward_pool[i]\n\n                probs = policy_net(state)\n                m = Categorical(probs)\n                loss = -m.log_prob(action) * reward  # Negtive score function x reward\n                loss.backward()\n\n            policy_optimizer.step()\n            \n            \n            state_pool = []\n            action_pool = []\n            reward_pool = []\n            steps = 0\n\n\nif __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2022-11-11T18:06:01.813222Z","iopub.execute_input":"2022-11-11T18:06:01.813609Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Episode: 3 \tDuration: 740\nEpisode: 4 \tDuration: 964\nEpisode: 5 \tDuration: 743\nEpisode: 19 \tDuration: 957\nEpisode: 29 \tDuration: 668\nEpisode: 30 \tDuration: 424\nEpisode: 31 \tDuration: 970\nEpisode: 32 \tDuration: 786\nEpisode: 33 \tDuration: 856\nEpisode: 34 \tDuration: 526\nEpisode: 38 \tDuration: 673\nEpisode: 41 \tDuration: 894\nEpisode: 43 \tDuration: 830\nEpisode: 46 \tDuration: 469\nEpisode: 47 \tDuration: 248\nEpisode: 48 \tDuration: 353\nEpisode: 49 \tDuration: 338\nEpisode: 50 \tDuration: 316\nEpisode: 51 \tDuration: 167\nEpisode: 52 \tDuration: 178\nEpisode: 53 \tDuration: 169\nEpisode: 54 \tDuration: 178\nEpisode: 55 \tDuration: 186\nEpisode: 56 \tDuration: 360\nEpisode: 57 \tDuration: 186\nEpisode: 58 \tDuration: 164\nEpisode: 59 \tDuration: 157\nEpisode: 60 \tDuration: 182\nEpisode: 61 \tDuration: 196\nEpisode: 62 \tDuration: 196\nEpisode: 63 \tDuration: 183\nEpisode: 64 \tDuration: 156\nEpisode: 65 \tDuration: 166\nEpisode: 66 \tDuration: 231\nEpisode: 67 \tDuration: 164\nEpisode: 68 \tDuration: 212\nEpisode: 69 \tDuration: 196\nEpisode: 70 \tDuration: 181\nEpisode: 71 \tDuration: 206\nEpisode: 72 \tDuration: 167\nEpisode: 73 \tDuration: 131\nEpisode: 74 \tDuration: 149\nEpisode: 75 \tDuration: 197\nEpisode: 76 \tDuration: 263\nEpisode: 77 \tDuration: 186\nEpisode: 78 \tDuration: 192\nEpisode: 79 \tDuration: 195\nEpisode: 80 \tDuration: 191\nEpisode: 81 \tDuration: 375\nEpisode: 82 \tDuration: 279\nEpisode: 83 \tDuration: 239\nEpisode: 84 \tDuration: 223\nEpisode: 85 \tDuration: 245\nEpisode: 86 \tDuration: 443\nEpisode: 87 \tDuration: 487\nEpisode: 90 \tDuration: 258\nEpisode: 91 \tDuration: 242\nEpisode: 93 \tDuration: 926\nEpisode: 94 \tDuration: 713\nEpisode: 95 \tDuration: 603\nEpisode: 96 \tDuration: 527\nEpisode: 97 \tDuration: 197\nEpisode: 98 \tDuration: 845\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.figure()\nplt.plot(episode_rewards)\nplt.savefig(\"lr-1.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}